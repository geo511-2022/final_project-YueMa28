---
title: "what does time tell"
author: Yue Ma
subtitle: Using time series data to train deep learning models for classification
---

# Introduction #I like the layout of your code. logically, it is easy to follow. 

Time series data is of tremendous importance in the field of environmental sustainability. Exploring this kind of data helps us understand the pattern of ecosystems, which allows us to detect the abnormal changes in time and help the relative department to make decisions. In this study, I will compare the performances of two state-of-the-art time series classification (TSC) models, fully convectional neural network (FCN) and deep Residual Network (ResNet).

# Materials and methods

This study can be divided into four parts:

1) data collection and pre-processing: 
In this part, I collected the aforementioned data and normalized them into the same value range (Except the NDVI data). The normalized value range for the NDVI data is [-1,1], while the normalized value range for the other environmental variables is (0,1]. 

2) build training and testing data sets:
Based the land cover classification data, I select pixels that do not change during the study period and build a mask based on them. This mask is then applied to select pixels from the NDVI and environmental variables data. Then the label from the classification data and the data from previous step are combined together and then splited into training and testing datasets.

3) build and train deep learning models:
The deep learning models used in this paper is proposed in XXX Wang's paper in 2017. In this paper, I keep the original structures of the models in the paper and 

4) discussion:
Based on the results I get from the previous step, the performances of these two models are analyzed and the results have referential significance to relative studies.

![](https://github.com/geo511-2022/final_project-YueMa28/blob/master/flowwork.png)

## Before the code

In this study I planned to use ten years as the study period, however the large size of dataset would run into 'Error: vector memory exhausted (limit reached?)' Error. So on this website, I will only use one year as the study period. If you are interested in the original code, please copy and paste the commented parts of code and run them on a powerful server.

## Demo (using data from 2018 as an example)

Here I list all the packages that I used in this study.

```{r, message=F, warning=F}
library(tensorflow)
library(raster)
library(lubridate)
library(keras)
library(sp)
library(timechange)
library(plot3D)
library(ggplot2)
library(piggyback)
library(dst)
library(reticulate)
library(countcolors)
library(tidyterra)
library(downloader)
library(sf)

```

## Creat time series to collect data from github
In this part we collected the time series data from Dr. Wilson's github.



```{r}

#year_list = c('2009','2010','2011','2012','2013','2014','2015','2016','2017','2018')

#date_list = list() 

#count <- 1
#for(i in year_list){ 
#  ss <- as.Date(paste(i,'-01-01',sep=''))
#  dates <- seq(from=ss, by=32, length.out=12)
#  for(j in 1:12){
#    temp <- format(dates[[j]], format = '%Y_%m_%d')
#   date_list[[count]] <- temp
#    count <- count + 1
#  }
#} 

#date_list

date_list = list() 

ss <- as.Date(paste(2018,'-01-01',sep=''))
dates <- seq(from=ss, by=32, length.out=12)
for(j in 1:12){
  temp <- format(dates[[j]], format = '%Y_%m_%d')
  date_list[[j]] <- temp
}

date_list


```


## Download and clean all required data
Here, I load all the NDVI data and compact them as one three-dimensional array. I firstly go through the data to check if there is any N.A or infinity values in the array. If there are infinity values, replace them with N.A values. 


# download the data
In this part I will show the code for downloading the data from Dr. Wilson's github. I also uploaded the data in the release so this download code is commented out. Feel free to comment it in and use it if you want to download the data by yourself.

```{r}
#ndvi_path <- "the place where you save your NDVI data"
#env_path <- "the place where you save your env data"

#download the NDVI data from Dr.Wilson's github
#for(i in 1:12){
#  sample_data <- pb_download(paste(date_list[[i]],'.tif',sep = ''),
#            repo = "AdamWilsonLab/emma_envdata",
#            tag = "raw_ndvi_modis",
#            dest = file.path(ndvi_path)
#            )
#}

#download the env data

#dem_data <- pb_download('nasadem.tif',
#            repo = "AdamWilsonLab/emma_envdata",
#            tag = "processed_static",
#            dest = file.path(env_path)
#            )

#soil_data <- pb_download('soil_Total_N_.tif',
#            repo = "AdamWilsonLab/emma_envdata",
#            tag = "processed_static",
#            dest = file.path(env_path)
#            )

#conc_data <- pb_download('MODCF_seasonality_concentration.tif',
#            repo = "AdamWilsonLab/emma_envdata",
#            tag = "processed_static",
#            dest = file.path(env_path)
#            )

```

# Collect NDVI data

This part collected NDVI data from January to December 2018 and visualize them.

```{r}
NDVI_filelist = c()

for(i in 1:12){
  NDVI_filelist[i] = paste('https://github.com/geo511-2022/final_project-YueMa28/releases/download/NDVI/',date_list[i],'.tif' ,sep="")
}

ndvi_rasters <- terra::rast(NDVI_filelist)
names(ndvi_rasters) <- c("Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec")


ggplot() +
  geom_spatraster(data=ndvi_rasters) +
  facet_wrap(~lyr, ncol = 4) 

```
This part normalizes the NDVI data from original value range ([80,200]) to new value range ([-1,1]).  

```{r}
ndvi_rasters_norm <- (ndvi_rasters-100)/100

ggplot() +
  geom_spatraster(data=ndvi_rasters_norm) +
  facet_wrap(~lyr, ncol = 4) 

```
This part calculate the exact value range of this normalized NDVI data.
```{r}
ndvi_arr_norm = as.array(ndvi_rasters_norm)

print(paste("There are nan values in NDVI data: ",as.character(NaN %in% ndvi_arr_norm,seq='')))
print(paste("There are infinitive values in NDVI data: ",as.character(Inf %in% ndvi_arr_norm,seq='')))

print(paste("The min value of NDVI data (without nan): ",min(ndvi_arr_norm,na.rm = TRUE)))
print(paste("The max value of NDVI data (without nan): ",max(ndvi_arr_norm,na.rm = TRUE)))

```
```{r}
image(ndvi_arr_norm[nrow(ndvi_arr_norm):1,,1],asp=1)

```
# collect the environmental data
The environmental data have been already downloaded and uploaded in the release. Here is the code for loading them.
```{r}
env_filelist = c()

env_filelist[1] = "https://github.com/geo511-2022/final_project-YueMa28/releases/download/ENV/nasadem.tif"
env_filelist[2] = "https://github.com/geo511-2022/final_project-YueMa28/releases/download/ENV/soil_Total_N_.tif"
env_filelist[3] = "https://github.com/geo511-2022/final_project-YueMa28/releases/download/ENV/MODCF_seasonality_concentration.tif"

env_rasters <- terra::rast(env_filelist)
names(env_rasters) <- c("dem","soil","concentration")

ggplot() +
  geom_spatraster(data=env_rasters) +
  facet_wrap(~lyr, ncol = 3) 

```

```{r}
env_arr_ori = as.array(env_rasters)

print(paste("There are nan values in static data: ",as.character(NaN %in% env_arr_ori,seq='')))
print(paste("There are infinitive values in static data: ",as.character(Inf %in% env_arr_ori,seq='')))

print(paste("The min value of static data (without nan): ",min(env_arr_ori,na.rm = TRUE)))
print(paste("The max value of static data (without nan): ",max(env_arr_ori,na.rm = TRUE)))
```


Here we normalized the environmental data and visualize them.
```{r}
env_rasters_norm1 <- terra::rast()

dem_rasters <- terra::rast(env_filelist[1])
dem_min <- terra::minmax(dem_rasters)[1]
dem_max <- terra::minmax(dem_rasters)[2]
dem_rasters_norm <- (dem_rasters - dem_min + 1) / (dem_max - dem_min + 2)


soil_rasters <- terra::rast(env_filelist[2])
soil_min <- terra::minmax(soil_rasters)[1]
soil_max <- terra::minmax(soil_rasters)[2]
soil_rasters_norm <- (soil_rasters - soil_min + 1) / (soil_max - soil_min + 2)


conc_rasters <- terra::rast(env_filelist[3])
conc_min <- terra::minmax(conc_rasters)[1]
conc_max <- terra::minmax(conc_rasters)[2]
conc_rasters_norm <- (conc_rasters - conc_min + 1) / (conc_max - conc_min + 2)

env_rasters_norm <- c(dem_rasters_norm,soil_rasters_norm,conc_rasters_norm)
names(env_rasters_norm) <- c("dem","soil","concentration")

ggplot() +
  geom_spatraster(data=env_rasters_norm) +
  facet_wrap(~lyr, ncol = 3) 


```
Here we explore the detailed information about the environmental data.
```{r}
env_arr_norm = as.array(env_rasters_norm)

print(paste("There are nan values in static data: ",as.character(NaN %in% env_arr_norm,seq='')))
print(paste("There are infinitive values in static data: ",as.character(Inf %in% env_arr_norm,seq='')))

print(paste("The min value of static data (without nan): ",min(env_arr_norm,na.rm = TRUE)))
print(paste("The max value of static data (without nan): ",max(env_arr_norm,na.rm = TRUE)))

```

# load the dynamic data

The precipitation (pr), radiation (rsds) and temperature (tas) data have been uploaded in the release part.

```{r}
pr_filelist = c()
month <- c("01","02","03","04","05","06","07","08","09","10","11","12")

for(i in month){
  pr_filelist[i] = paste('https://github.com/geo511-2022/final_project-YueMa28/releases/download/dynamic_pr/pr_',i,'_2018.tif' ,sep="")
}

pr_rasters <- terra::rast(pr_filelist)
names(pr_rasters) <- c("Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec")
pr_rasters=terra::project(pr_rasters,ndvi_rasters_norm)

ggplot() +
  geom_spatraster(data=pr_rasters) +
  facet_wrap(~lyr, ncol = 4) 

```

```{r}
prec_arr_ori <- as.array(pr_rasters)

print(paste("There are nan values in precipitation data: ",as.character(NaN %in% prec_arr_ori,seq='')))
print(paste("There are infinitive values in precipitation data: ",as.character(Inf %in% prec_arr_ori,seq='')))

print(paste("The min value of precipitation data (without nan): ",min(prec_arr_ori,na.rm = TRUE)))
print(paste("The max value of precipitation data (without nan): ",max(prec_arr_ori,na.rm = TRUE)))

```

Here we normalize the precipitation data and visualize them.
```{r}
pr_rasters_norm1 <- terra::rast()

for(i in 1:11){
  current_raster <- terra::rast(pr_filelist[i])
  current_min <- terra::minmax(current_raster)[1]
  current_max <- terra::minmax(current_raster)[2]
  current_raster_norm <- (current_raster - current_min + 1) / (current_max - current_min + 2)
  current_raster_norm = terra::project(current_raster_norm,ndvi_rasters_norm)
  pr_rasters_norm1 <- c(pr_rasters_norm1,current_raster_norm)
}

current_raster <- terra::rast(pr_filelist[12])
current_min <- terra::minmax(current_raster)[1]
current_max <- terra::minmax(current_raster)[2]
current_raster_norm <- (current_raster - current_min + 1) / (current_max - current_min + 2)
current_raster_norm = terra::project(current_raster_norm,ndvi_rasters_norm)

pr_rasters_norm <- c(pr_rasters_norm1,current_raster_norm)
names(pr_rasters_norm) <- c("Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec")

ggplot() +
  geom_spatraster(data=pr_rasters_norm) +
  facet_wrap(~lyr, ncol = 4) 

```


Here we explore more about the normalized precipitation data.
```{r}
prec_arr_norm <- as.array(pr_rasters_norm)

print(paste("There are nan values in precipitation data: ",as.character(NaN %in% prec_arr_norm,seq='')))
print(paste("There are infinitive values in precipitation data: ",as.character(Inf %in% prec_arr_norm,seq='')))

print(paste("The min value of precipitation data (without nan): ",min(prec_arr_norm,na.rm = TRUE)))
print(paste("The max value of precipitation data (without nan): ",max(prec_arr_norm,na.rm = TRUE)))

```

Here we load the rsds data and visualize them.
```{r}

rsds_filelist = c()
month <- c("01","02","03","04","05","06","07","08","09","10","11","12")

for(i in month){
  rsds_filelist[i] = paste('https://github.com/geo511-2022/final_project-YueMa28/releases/download/dynamic_rsds/rsds_',i,'_2018.tif' ,sep="")
}

rsds_rasters <- terra::rast(rsds_filelist)
names(rsds_rasters) <- c("Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec")
rsds_rasters = terra::project(rsds_rasters,ndvi_rasters_norm)

ggplot() +
  geom_spatraster(data=rsds_rasters) +
  facet_wrap(~lyr, ncol = 4) 
```


Here we explore the details of the original data.
```{r}
rsds_arr_ori <- as.array(rsds_rasters)
print(paste("There are nan values in downwelling shortwave flux data: ",as.character(NaN %in% rsds_arr_ori,seq='')))
print(paste("There are infinitive values in downwelling shortwave flux data: ",as.character(Inf %in% rsds_arr_ori,seq='')))

print(paste("The min value of downwelling shortwave flux data (without nan): ",min(rsds_arr_ori,na.rm = TRUE)))
print(paste("The max value of downwelling shortwave flux data (without nan): ",max(rsds_arr_ori,na.rm = TRUE)))

```

```{r}
rsds_rasters_norm1 <- terra::rast()

for(i in 1:11){
  current_raster <- terra::rast(rsds_filelist[i])
  current_min <- terra::minmax(current_raster)[1]
  current_max <- terra::minmax(current_raster)[2]
  current_raster_norm <- (current_raster - current_min + 1) / (current_max - current_min + 2)
  current_raster_norm = terra::project(current_raster_norm,ndvi_rasters_norm)
  rsds_rasters_norm1 <- c(rsds_rasters_norm1,current_raster_norm)
}

current_raster <- terra::rast(rsds_filelist[12])
current_min <- terra::minmax(current_raster)[1]
current_max <- terra::minmax(current_raster)[2]
current_raster_norm <- (current_raster - current_min + 1) / (current_max - current_min + 2)
current_raster_norm = terra::project(current_raster_norm,ndvi_rasters_norm)
rsds_rasters_norm <- c(rsds_rasters_norm1,current_raster_norm)
names(rsds_rasters_norm) <- c("Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec")

ggplot() +
  geom_spatraster(data=rsds_rasters_norm) +
  facet_wrap(~lyr, ncol = 4) 
```



```{r}
rsds_arr_norm <- as.array(rsds_rasters_norm)
print(paste("There are nan values in downwelling shortwave flux data: ",as.character(NaN %in% rsds_arr_norm,seq='')))
print(paste("There are infinitive values in downwelling shortwave flux data: ",as.character(Inf %in% rsds_arr_norm,seq='')))

print(paste("The min value of downwelling shortwave flux data (without nan): ",min(rsds_arr_norm,na.rm = TRUE)))
print(paste("The max value of downwelling shortwave flux data (without nan): ",max(rsds_arr_norm,na.rm = TRUE)))

```

```{r}
tas_filelist = c()
month <- c("01","02","03","04","05","06","07","08","09","10","11","12")

for(i in month){
  tas_filelist[i] = paste('https://github.com/geo511-2022/final_project-YueMa28/releases/download/dynamic_tas/tas_',i,'_2018.tif' ,sep="")
}

tas_rasters <- terra::rast(tas_filelist)
names(tas_rasters) <- c("Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec")

tas_rasters = terra::project(tas_rasters,ndvi_rasters_norm)
ggplot() +
  geom_spatraster(data=tas_rasters) +
  facet_wrap(~lyr, ncol = 4) 

```


```{r}
tas_arr_ori <- as.array(tas_rasters)
print(paste("There are nan values in mean daily airtemperature data: ",as.character(NaN %in% tas_arr_ori,seq='')))
print(paste("There are infinitive values in mean daily air temperature data: ",as.character(Inf %in% tas_arr_ori,seq='')))

print(paste("The min value of mean daily air temperature data (without nan): ",min(tas_arr_ori,na.rm = TRUE)))
print(paste("The max value of mean daily air temperature data (without nan): ",max(tas_arr_ori,na.rm = TRUE)))

```

```{r}
tas_rasters_norm1 <- terra::rast()

for(i in 1:11){
  current_raster <- terra::rast(tas_filelist[i])
  current_min <- terra::minmax(current_raster)[1]
  current_max <- terra::minmax(current_raster)[2]
  current_raster_norm <- (current_raster - current_min + 1) / (current_max - current_min + 2)
  current_raster_norm = terra::project(current_raster_norm,ndvi_rasters_norm)
  tas_rasters_norm1 <- c(tas_rasters_norm1,current_raster_norm)
}

current_raster <- terra::rast(tas_filelist[12])
current_min <- terra::minmax(current_raster)[1]
current_max <- terra::minmax(current_raster)[2]
current_raster_norm <- (current_raster - current_min + 1) / (current_max - current_min + 2)
current_raster_norm = terra::project(current_raster_norm,ndvi_rasters_norm)
tas_rasters_norm <- c(tas_rasters_norm1,current_raster_norm)
names(tas_rasters_norm) <- c("Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec")

ggplot() +
  geom_spatraster(data=tas_rasters_norm) +
  facet_wrap(~lyr, ncol = 4) 
```



```{r}
tas_arr_norm <- as.array(tas_rasters_norm)
print(paste("There are nan values in mean daily airtemperature data: ",as.character(NaN %in% tas_arr_norm,seq='')))
print(paste("There are infinitive values in mean daily air temperature data: ",as.character(Inf %in% tas_arr_norm,seq='')))

print(paste("The min value of mean daily air temperature data (without nan): ",min(tas_arr_norm,na.rm = TRUE)))
print(paste("The max value of mean daily air temperature data (without nan): ",max(tas_arr_norm,na.rm = TRUE)))
```


```{r}

#pr_rasters_norm=terra::project(pr_rasters_norm,ndvi_rasters_norm)

all_variables_raster <- c(ndvi_rasters_norm,env_rasters_norm,pr_rasters_norm,rsds_rasters_norm,tas_rasters_norm)

```
This part generate the label of the training dataset.
```{r}
land_cover_type <- terra::rast("https://github.com/geo511-2022/final_project-YueMa28/releases/download/landcover_type/clipped_1990_2020.tif")

land_cover_type=terra::project(land_cover_type,ndvi_rasters_norm)

land_cover_arr <- as.array(land_cover_type)
```


```{r}
binary_mask <- array(dim=c(1634,2035))

for(i in 1:1634){
  for(j in 1:2035){
    if(is.nan(land_cover_arr[i,j,1])){
      binary_mask[i,j] = NaN
    }
    else if(land_cover_arr[i,j,1] == 105){
      binary_mask[i,j] = 1
    }
    else{
      binary_mask[i,j] = 0
    }
  }
}

image(binary_mask,asp=1)

```




```{r}
#all_variables_arr <- array(dim=c(1634,2035,12,7))

all_variables_mask <- array(dim=c(1634,2035))

count_1 <- 0

for(i in 1:1634){
  for(j in 1:2035){
    for(k in 1:12){
      if(is.nan(ndvi_arr_norm[i,j,k])){
        all_variables_mask[i,j] = -1
        break
      }
      else if(is.nan(prec_arr_norm[i,j,k])){
        all_variables_mask[i,j] = -1
        break
      }
      else if(is.nan(rsds_arr_norm[i,j,k])){
        all_variables_mask[i,j] = -1
        break
      }
      else if(is.nan(tas_arr_norm[i,j,k])){
        all_variables_mask[i,j] = -1
        break
      }
      else{
        all_variables_mask[i,j] = 1
        count_1 <- count_1 + 1
      }
    for(h in 1:3){
      if(is.nan(env_arr_norm[i,j,h])){
        all_variables_mask[i,j] = -1
        break
      }
      else{
        all_variables_mask[i,j] = 1
        count_1 <- count_1 + 1
      }
    }
    }
  }
}

image(all_variables_mask,asp=1)
```


```{r}
combine_mask <- array(dim=c(1634,2035))

for(m in 1:1634){
  for(n in 1:2035){
    if(is.nan(all_variables_mask[m,n]) || is.nan(binary_mask[m,n])){
      combine_mask[m,n] = -1
    }
    else if(all_variables_mask[m,n] == 1 && binary_mask[m,n] == 1){
      combine_mask[m,n] = 1
    }
    else{
      combine_mask[m,n] = -1
    }
  }
}

image(combine_mask,asp=1)

#1 %in% all_variables_mask

#valid_number <- 0
#for(i in 1:1634){
#  for(j in 1:2035){
#    if(combine_mask[i,j]==1){
#      valid_number = valid_number + 1
#    }
#    else{
#      next
#    }
#  }
#}
#valid_number
#train_number = as.integer(valid_number*0.75)
#test_number = as.integer(valid_number*0.25)
```


```{r}
lab_dict = c('20.0'=0,
             '30.0'=1,
             '41.0'=2,
             '42.0'=3,
             '43.0'=4,
             '50.0'=5,
             '60.0'=6,
             '80.0'=7,
             '90.0'=8,
             '112.0'=9,
             '114.0'=10,
             '116.0'=11,
             '124.0'=12,
             '126.0'=13)
```


```{r}
#total_dataset = array(c(valid_number,120,7))

#total_label = array(c(valid_number,1))

#total_dataset_count = 1

#for(i in 1:1634){
#  for(j in 1:2035){
#    if(combine_mask[i,j] == 1){
#      total_dataset[total_dataset_count,,] = all_variables[i,j,,]
#      trans_lab = lab_dict[[as.character(class_arr[i,j,1])]]
#      total_label[total_dataset_count,1] = trans_lab
#      total_dataset_count = total_dataset_count+1
#    }
#  }
#}



total_dataset = array(dim=c(valid_number,12,7))

total_label = array(dim=c(valid_number,1))

total_dataset_count = 1

for(i in 1:1634){
  for(j in 1:2035){
    if(combine_mask[i,j] == 1){
      total_dataset[total_dataset_count,,] = all_variables[i,j,,]
      trans_lab = lab_dict[[as.character(class_arr[i,j,1])]]
      total_label[total_dataset_count,1] = trans_lab
      total_dataset_count = total_dataset_count+1
    }
  }
}
```


```{r}
valid_number
total_label

set.seed(10000)
random_int <- sample(1:valid_number, train_number, replace = FALSE)
train_dataset = list()
train_label = list()
test_dataset = list()
test_label = list()

total_train_num <- 1
total_test_num <- 1
for(i in 1:valid_number){
  if(i %in% random_int){
    train_dataset[[total_train_num]] <- total_dataset[,i,]
    train_label[[total_train_num]] <- total_label[i,]
    total_train_num = total_train_num + 1
  }
  else{
    test_dataset[[total_test_num]] <- total_dataset[,i,]
    test_label[[total_test_num]] <- total_label[i,]
    total_test_num = total_test_num + 1
  }
}

```
## build the training and testing dataset

Load the land cover classification dataset and compare the value of each pixel from year to year. Save the location of pixels which don't change during the study period and build a mask based on these pixels.

Use the mask to filter out the pixels in the compacted time series dataset. Then filter out all the pixels who have 0 in their environmental variables.

Convert the 3-dimensional dataset to 2-dimensional dataset. After this, add labels to the 2-dimensional dataset. Then use continuous numbers to replace the original label numbers.

Randomly split the dataset (75%-25%) as training dataset and testing dataset.

## build and train the model
Build the FCN model and train it with the training dataset.

Build the ResNet model and train it with the training dataset.

Test the models with testing dataset, and evaluate them.
```{r}
FCN_model <- keras_model_sequential()
FCN_model %>%
  layer_input(input_shape=c(120,7,1)) %>%
  layer_conv_2d(128,5,1,padding = 'same') %>%
  layer_batch_normalization() %>%
  layer_activation('relu') %>%
  layer_conv_2d(256,5,1,padding = 'same') %>%
  layer_batch_normalization() %>%
  layer_activation('relu') %>%
  layer_conv_2d(128,3,1,padding = 'same') %>%
  layer_batch_normalization() %>%
  layer_activation('relu') %>%
  layer_average_pooling_2d() %>%
  layer_dense(14,activation = 'softmax')

summary(FCN_model)

```

# Results

[~200 words]

Tables and figures (maps and other graphics) are carefully planned to convey the results of your analysis. Intense exploration and evidence of many trials and failures. The author looked at the data in many different ways before coming to the final presentation of the data.

Show tables, plots, etc. and describe them.






# Conclusions

[~200 words]

Clear summary adequately describing the results and putting them in context. Discussion of further questions and ways to continue investigation.

# References

All sources are cited in a consistent manner
